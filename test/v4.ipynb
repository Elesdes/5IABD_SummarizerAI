{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T23:03:14.771413200Z",
     "start_time": "2023-12-04T23:03:11.633062100Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n",
    "import PyPDF2\n",
    "import requests\n",
    "import io\n",
    "import nltk\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T23:03:14.866118700Z",
     "start_time": "2023-12-04T23:03:14.775923500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/elesdes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF2Text | _Understanding the Formation of Galaxies with Warm Dark Matter_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T23:03:16.280114200Z",
     "start_time": "2023-12-04T23:03:14.864121400Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/pdf/2310.06882.pdf\"\n",
    "response = requests.get(url)\n",
    "pdf_reader = PyPDF2.PdfReader(io.BytesIO(response.content))\n",
    "text = \"\".join(\n",
    "    [\n",
    "        pdf_reader.pages[page_num].extract_text()\n",
    "        for page_num in range(len(pdf_reader.pages))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T23:03:16.287142500Z",
     "start_time": "2023-12-04T23:03:16.284126500Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_text_into_batches(text, batch_size=4096):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    current_batch = []\n",
    "    current_batch_size = 0\n",
    "    batches = []\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        if current_batch_size + len(sentence) <= batch_size:\n",
    "            current_batch.append(sentence)\n",
    "            current_batch_size += len(sentence)\n",
    "        else:\n",
    "            batches.append(\" \".join(current_batch))\n",
    "            current_batch = [sentence]\n",
    "            current_batch_size = len(sentence)\n",
    "\n",
    "    if current_batch:\n",
    "        batches.append(\" \".join(current_batch))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T23:03:16.294184500Z",
     "start_time": "2023-12-04T23:03:16.287142500Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_batches(batches, tokenizer, model):\n",
    "    predictions = []\n",
    "\n",
    "    for batch in tqdm(batches):\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\")  # Max length = 4096\n",
    "        prediction = model.generate(**inputs)\n",
    "        prediction = tokenizer.batch_decode(prediction, skip_special_tokens=True)\n",
    "        predictions.extend(prediction)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization | _BigBird Pegasus Large - Arxiv Dataset Variation_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T23:03:22.080897Z",
     "start_time": "2023-12-04T23:03:16.295183700Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"google/bigbird-pegasus-large-arxiv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\n",
    "    model_name, attention_type=\"original_full\", block_size=32, num_random_blocks=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T23:09:21.729066700Z",
     "start_time": "2023-12-04T23:03:22.082902600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:00<00:00, 1398101.33it/s]\n",
      "100%|██████████| 8/8 [06:31<00:00, 48.94s/it]\n"
     ]
    }
   ],
   "source": [
    "batches = split_text_into_batches(text)\n",
    "predictions = process_batches(batches, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Summary | _Post-Processing_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_text(input_text):\n",
    "    cleaned_text = re.sub(r\"<s>\", \"\", input_text)\n",
    "    cleaned_text = \" \".join(cleaned_text.split())\n",
    "    return re.sub(\n",
    "        r\"(?<=\\. )(\\w)|^\\w\", lambda match: match.group(0).capitalize(), cleaned_text\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = [prediction.split(\"<n>\")[0] for prediction in predictions]\n",
    "summary = \"\\n\".join(summary)\n",
    "summary = clean_text(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/v4/summary.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5IABD_SummarizerTool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
